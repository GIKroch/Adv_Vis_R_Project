{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "# Stanford NLP library \n",
    "# https://stanfordnlp.github.io/stanfordnlp/installation_usage.html\n",
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which extract only words from joined files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): \n",
    "    return re.findall(r'[a-zA-Z]+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Counter dictionary, it shows summed up number of all words which occur in specific joined file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\\17th_joined_file.txt\n",
      "C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\\18th_joined_file.txt\n",
      "C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\\19th_joined_file.txt\n",
      "C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\\20th_joined_file.txt\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\"\n",
    "counter_dict = {}\n",
    "for century in os.listdir(base_path):\n",
    "    century_path = base_path + \"\\{}\".format(century)\n",
    "    counter_dict[century] = pl_books = Counter(words(open(century_path, encoding = 'utf-8').read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for analysis of century specific files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "century_text = counter_dict[\"17th_joined_file.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating dataframe out of dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for word, number in century_text.items():\n",
    "    data.append((word, number))\n",
    "\n",
    "df = pd.DataFrame(data, columns = [\"Word\", \"Number of occurences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = \"Number of occurences\", ascending = False)\n",
    "df.reset_index(inplace = True)\n",
    "df.drop(columns = [\"index\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce lemmatization to restrain number of words for future steps like POS-tagging or Sentiment Analysis. Different variations of the same word do not bring us any interesting information in the area of our study so it is better to cut down unnecessary diversity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f is for progress bar\n",
    "def get_lemma(text, f):\n",
    "    f.value += 1\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            return word.lemma\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dcd533326a41c09d4d6852bc922e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=35192)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = IntProgress(min= 0, max = len(df)) # instantiate the bar\n",
    "display(f)\n",
    "\n",
    "\n",
    "df[\"Lemma\"] = df[\"Word\"].apply(lambda text:\n",
    "                              get_lemma(text, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating lemmatized dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After process of lemmatization we can group our dataframe in a way that in further analysis we will be focused only on lemmatized versions of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemmatized = df.groupby(\"Lemma\").aggregate(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemmatized.sort_values(by = \"Number of occurences\", ascending = False, inplace = True)\n",
    "df_lemmatized.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when our dataset has been limited we can conduct further actions (e.g. POS-tagging and sentiment analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f is for progress bar\n",
    "def get_part_of_speech(text, f):\n",
    "    f.value += 1\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            return word.upos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0642f6eca42347dd8eea7fc2ac9d103e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=27327)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = IntProgress(min= 0, max = len(df_lemmatized)) # instantiate the bar\n",
    "display(f)\n",
    "df_lemmatized[\"Part_of_speech\"] = df_lemmatized[\"Lemma\"].apply(lambda text: get_part_of_speech(text, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score = pd.DataFrame(list(df_lemmatized[\"Lemma\"].apply(lambda text: sid.polarity_scores(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemmatized = df_lemmatized.join(sentiment_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations above but for general purpose, all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\grzeg\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\grzeg\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\grzeg\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\grzeg\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "# Stanford NLP library \n",
    "# https://stanfordnlp.github.io/stanfordnlp/installation_usage.html\n",
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data \n",
    "\n",
    "def words(text): \n",
    "    return re.findall(r'[a-zA-Z]+', text.lower())\n",
    "\n",
    "base_path = r\"C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\"\n",
    "counter_dict = {}\n",
    "for century in os.listdir(base_path):\n",
    "    century_path = base_path + \"\\{}\".format(century)\n",
    "    counter_dict[century] = pl_books = Counter(words(open(century_path, encoding = 'utf-8').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "#### f is for progress bar\n",
    "def get_lemma(text, f):\n",
    "    f.value += 1\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            return word.lemma\n",
    "\n",
    "# Part Of Speech tagging\n",
    "\n",
    "#### f is for progress bar\n",
    "def get_part_of_speech(text, f):\n",
    "    f.value += 1\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            return word.upos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(key):\n",
    "    print(key)\n",
    "    century_text = counter_dict[key]\n",
    "    \n",
    "    ## Creating dataframe out of counter dictionary \n",
    "    data = []\n",
    "    for word, number in century_text.items():\n",
    "        data.append((word, number))\n",
    "\n",
    "    df = pd.DataFrame(data, columns = [\"Word\", \"Number of occurences\"])\n",
    "    \n",
    "    \n",
    "    ## Sorting df\n",
    "    df = df.sort_values(by = \"Number of occurences\", ascending = False)\n",
    "    df.reset_index(inplace = True)\n",
    "    df.drop(columns = [\"index\"], inplace = True)\n",
    "    \n",
    "    \n",
    "    ## Lemmatization \n",
    "    \n",
    "    f = IntProgress(min= 0, max = len(df))\n",
    "    display(f)\n",
    "\n",
    "\n",
    "    df[\"Lemma\"] = df[\"Word\"].apply(lambda text:\n",
    "                              get_lemma(text, f))\n",
    "    \n",
    "    ## Creating Lemmatized Dataset\n",
    "    \n",
    "    df_lemmatized = df.groupby(\"Lemma\").aggregate(\"sum\")\n",
    "    df_lemmatized.sort_values(by = \"Number of occurences\", ascending = False, inplace = True)\n",
    "    df_lemmatized.reset_index(inplace = True)\n",
    "    \n",
    "    ## POS-tagging\n",
    "    f = IntProgress(min= 0, max = len(df_lemmatized)) # instantiate the bar\n",
    "    display(f)\n",
    "    df_lemmatized[\"Part_of_speech\"] = df_lemmatized[\"Lemma\"].apply(lambda text: get_part_of_speech(text, f))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Sentiment Analysis\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = pd.DataFrame(list(df_lemmatized[\"Lemma\"].apply(lambda text: sid.polarity_scores(text))))\n",
    "    df_lemmatized = df_lemmatized.join(sentiment_score)\n",
    "    \n",
    "    \n",
    "    ## Saving data\n",
    "    file_name = key.replace(\".txt\", \".xlsx\")\n",
    "    df_lemmatized.to_excel(file_name, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17th_joined_file.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3f264ba3744675a506ecbb71723710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=30564)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c48a14126745f1a41d6ddaf99fa4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=23010)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18th_joined_file.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446a515849004712950d50bd7b8a84e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=58719)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2389a9bf2de4c9da8964292d065e063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=47978)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19th_joined_file.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed994f93d3cc4ece89395c36997ea9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=40645)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c02e67e83245d58fd5046a59a866d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=29909)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20th_joined_file.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e345550d874b3f97d5cf40cf54e8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=25010)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201785d45f344a55a373f2bbf7f6f8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=18246)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key in counter_dict.keys():\n",
    "    prepare_dataset(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
