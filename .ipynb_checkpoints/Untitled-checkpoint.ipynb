{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "# Stanford NLP library \n",
    "# https://stanfordnlp.github.io/stanfordnlp/installation_usage.html\n",
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which extract only words from joined files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): \n",
    "    return re.findall(r'[a-zA-Z]+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Counter dictionary, it shows summed up number of all words which occur in specific joined file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\\17th_joined_file.txt\n",
      "C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\\18th_joined_file.txt\n",
      "C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\\19th_joined_file.txt\n",
      "C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\\20th_joined_file.txt\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\grzeg\\Desktop\\studia\\Data Science\\2 rok\\semestr 1\\Advanced_VisualisationR\\projekt\\Adv_Vis_R_Project\\Joined_files\"\n",
    "counter_dict = {}\n",
    "for century in os.listdir(base_path):\n",
    "    century_path = base_path + \"\\{}\".format(century)\n",
    "    counter_dict[century] = pl_books = Counter(words(open(century_path, encoding = 'utf-8').read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for analysis of century specific files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "century_text = counter_dict[\"17th_joined_file.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating dataframe out of dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for word, number in century_text.items():\n",
    "    data.append((word, number))\n",
    "\n",
    "df = pd.DataFrame(data, columns = [\"Word\", \"Number of occurences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = \"Number of occurences\", ascending = False)\n",
    "df.reset_index(inplace = True)\n",
    "df.drop(columns = [\"index\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting stopwords to simplify analysis. It is unnecessary to perform later actions like POS-tagging or sentiment analysis on stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# ## First you have to download stopwords with the code commented below\n",
    "# # nltk.download(\"stopwords\")\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_stopwords(text, stopwords):\n",
    "#     if text in stopwords:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Stopwords\"] = df[\"Word\"].apply(lambda text:\n",
    "#                                      find_stopwords(text, stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce lemmatization to restrain number of words for future steps like POS-tagging or Sentiment Analysis. Different variations of the same word do not bring us any interesting information in the area of our study so it is better to cut down unnecessary diversity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f is for progress bar\n",
    "def get_lemma(text, f):\n",
    "    f.value += 1\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            return word.lemma\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dcd533326a41c09d4d6852bc922e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=35192)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = IntProgress(min= 0, max = len(df)) # instantiate the bar\n",
    "display(f)\n",
    "\n",
    "\n",
    "df[\"Lemma\"] = df[\"Word\"].apply(lambda text:\n",
    "                              get_lemma(text, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating lemmatized dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After process of lemmatization we can group our dataframe in a way that in further analysis we will be focused only on lemmatized versions of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemmatized = df.groupby(\"Lemma\").aggregate(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemmatized.sort_values(by = \"Number of occurences\", ascending = False, inplace = True)\n",
    "df_lemmatized.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when our dataset has been limited we can conduct further actions (e.g. POS-tagging and sentiment analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f is for progress bar\n",
    "def get_part_of_speech(text, f):\n",
    "    f.value += 1\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            return word.upos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac7a91e5f6a43e2abbc57e9f7e639db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=27327)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = IntProgress(min= 0, max = len(df_lemmatized)) # instantiate the bar\n",
    "display(f)\n",
    "df[\"Part_of_speech\"] = df_lemmatized[\"Lemma\"].apply(lambda text: get_part_of_speech(text, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
